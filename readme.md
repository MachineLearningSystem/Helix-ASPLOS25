# ASPLOS'25 Helix
## Introduction 
Helix is a distributed system designed for high-throughput, low-latency large language model
serving across heterogeneous and potentially geo-distributed GPU clusters. This repository
contains the official implementation of both Helix's simulator and prototype system. Our paper
can be found here [https://arxiv.org/abs/2406.01566](https://arxiv.org/abs/2406.01566).

## Distributed LLM Serving Simulator Tutorial
The Helix simulator is a high-fidelity discrete-event simulator implemented in Python,
specifically designed for distributed LLM serving across heterogeneous and geo-distributed
GPU clusters. It provides detailed modeling and analysis of system behavior in complex
distributed environments.

### Installing Dependencies
We recommend using Python 3.11. To install the required dependencies, run the following command:
```bash
conda create -n helix python=3.11 -y && conda activate helix
pip install -e .
```
We use Gurobi as the MILP solver in the simulator, which requires a valid license. Please follow
the instructions on the [Gurobi website](https://www.gurobi.com/) to obtain a license.

### Running the Simulator
In this tutorial, we use an example of serving LLaMA-2 70B in a cluster with 24 machines to demonstrate
how to use Helix's simulator. The example is is located in `./examples/simulation`:
```bash
cd examples/simulation
```

### Step 1: Generate Configuration Files
First, we need to generate a cluster configuration file, which specifies the nodes and network connections
in the cluster. Run the following command to generate example cluster configuration files
```bash
python step1_gen_cluster.py
```
This Python script uses `FakeClusterGenerator` to generate a cluster with 24 machines located in a single
region (`config/single24.ini`) and `PartitionedClusterGenerator` to generate a cluster with 24 machines
located in three different regions (`config/3cluster24.ini`). For generation of other cluster topologies,
please refer to these two helper classes to implement your own generator. The simulator also needs a machine
profile file, which specifies the nic speed and vram size of machines. We provide such an example in
`config/machine_profile.ini`.

### Step 2: Finding Model Placement Plans
The second step is to find a model placement, which specifies the layers each compute node holds.
In our simulator, we implement our MILP-based model placement method (`ILPLayout`) and three
heuristic-based model placement methods as baselines. Let's first use the heuristic methods to
generate model placements:
```bash
python step2_model_placement.py petals
python step2_model_placement.py swarm
python step2_model_placement.py homogeneous
```
These heuristic-based methods run very fast. However, the quality of the model placement can not
be guaranteed. Then, we use Helix's MILP-based model placement planner to generate high-quality
model placements:
```bash
python step2_model_placement.py ilp
```
Notice that we set the max running time to 10 hours. However, you can interrupt with `ctrl + c` at
any time, and the best model placement will be saved (press `ctrl + c` only once, otherwise the
process will be killed by the os). On a laptop with 14 cores, we run the solver for 10 minutes to
find the solution in `layouts/ilp`.

After running model placement, we will get a `{method_name}_sol.ini`, which specifies the layers
each compute node holds, and a `simulator_cluster.ini`, which is the cluster description file that
will be used in the simulation. If you are using our MILP-based model placement planner, you will
also get `ilp_model.lp` and `ilp_solution.sol`, which are generated by Gurobi to store the LP
problem and the raw solution.

> **Tips:** If you accidentally press `ctrl + c` twice and kill the process before `simulator_cluster.ini`
> is generated, you still have the chance to recover the results. As long as you have `ilp_solution.sol`,
> you can generate `simulator_cluster.ini` and `ilp_sol.ini` using the method in `verify_ilp.py`.

### Step 3: Run Simulation
Finally, we can run the simulate to see how the model placement plan and request scheduling performs:
```bash
python step3_run_simulation.py offline maxflow
```
In this example, we use the model placement found by Helix's MILP-based model placement planner in Step 2.
We use Helix's MaxFlow-based request scheduler to schedule requests. New request will arrive when the
cluster is able to serve more, and the length distribution follows the Azure Conversation Dataset.
We refer to this setup as offline setup.

When running the above code, you will first see the model placement of the cluster (plotted by
`simulator.visualize_cluster()`):

![model_placement](examples/simulation/sim_files/maxflow_offline/model_placement.jpg "model_placement")

Then, the simulator will print out the max compute throughput and max flow of the cluster, both in
token/s. Max compute throughput is the max number of tokens the cluster can process in one second
when ignoring all network and GPU memory size limits. Max flow is a static estimation of the max
serving throughput of the cluster (counting both prompt and decode phase). These two numbers are
upper bounds of the real serving throughput.

```
Max compute throughput = 2803.860988272896
Max flow = 1289.5557702751464
```

At this point, the simulator has finished initialization and starts to run simulation. It will
periodically print out the status of all compute nodes in the cluster:

```
# -------------- Watch -------------- #
Last event time = 29.999846466400065
Next event time = 30.00046720600006
[Item] active queries: 61, finished queries 0.
[Item] KV-Cache:
Node Name: Real Used / Real Total | Expected Used / Expected Total | Expected > Real
	Compute-2: 145194/3004416 (5%) | 224264 / 3004416 (7%) | True
	Compute-3: 74812/1124224 (7%) | 126028 / 1124224 (11%) | True
	Compute-4: 66434/2031488 (3%) | 112132 / 2031488 (6%) | True
	Compute-5: 91731/1751008 (5%) | 163989 / 1751008 (9%) | True
	Compute-6: 77388/1577856 (5%) | 168198 / 1577856 (11%) | True
	Compute-7: 86831/1751008 (5%) | 167181 / 1751008 (10%) | True
	Compute-8: 261104/4630752 (6%) | 504594 / 4630752 (11%) | True
	Compute-9: 76990/1124224 (7%) | 126028 / 1124224 (11%) | True
	Compute-10: 74812/1124224 (7%) | 126028 / 1124224 (11%) | True
	Compute-11: 161100/3377472 (5%) | 294708 / 3377472 (9%) | True
	Compute-12: 234131/5048576 (5%) | 448528 / 5048576 (9%) | True
	Compute-13: 39431/1751008 (2%) | 63399 / 1751008 (4%) | True
	Compute-14: 53700/1124224 (5%) | 98236 / 1124224 (9%) | True
	Compute-15: 74812/1124224 (7%) | 126028 / 1124224 (11%) | True
	Compute-16: 149121/3004416 (5%) | 224264 / 3004416 (7%) | True
	Compute-17: 114433/1577856 (7%) | 168198 / 1577856 (11%) | True
	Compute-18: 104856/1577856 (7%) | 168198 / 1577856 (11%) | True
	Compute-19: 311635/4630752 (7%) | 504594 / 4630752 (11%) | True
	Compute-20: 91490/1751008 (5%) | 157150 / 1751008 (9%) | True
	Compute-21: 52280/1124224 (5%) | 89800 / 1124224 (8%) | True
	Compute-22: 33217/2485120 (1%) | 56066 / 2485120 (2%) | True
	Compute-23: 36396/1751008 (2%) | 61292 / 1751008 (4%) | True
	Compute-24: 76232/1124224 (7%) | 134464 / 1124224 (12%) | True
	Compute-25: 93975/1751008 (5%) | 171913 / 1751008 (10%) | True
Realtime bottleneck usage: 0.07252436217246694
Expected bottleneck usage: 0.1089658871820387
# ------------ End Watch ------------ #
```

The watch items include number of active queries and KV-cache usage on each node. Don't worry if
`Expected > Real` is `False` for some nodes. This only indicates that the KV-cache estimator
underestimated the usage. Since we have a `expected_kv_hwm`, in most cases the compute nodes will
not run out of memory. And sometimes you might see a log like this:
```
[SchedulerNode-11] Reject scheduler - out_nodes=[24], reasons=['Fail-KV']
A query is rejected due to low kv-cache in specific routes!
```
This indicates that the best route is not available because of insufficient memory. The scheduler
will temporarily reject the query (which is ok as we are in offline mode) and try to schedule it
later, when there is more memory available.

If the actual KV-cache usage is much larger than the estimation and causes the simulation to fail,
you can increase `expected_output_length_ratio` and decrease the `expected_kv_hwm` in `KVParameters`.
This will help the scheduler better estimate the real KV-cache usage.

Eventually, when the simulation finishes, the simulator will print out the statistics like this:
```
# ------------------------------------------------------------- #
Simulation Results (time range: 80s - 680s)
Avg decode speed: 252.1 tokens/s
Avg prompt latency: 2.955s
Avg decode latency: 1.450s
# ------------------------------------------------------------- #
```
This log shows the average decode throughput, average prompt latency and average decode latency in
this setup. You will also see another log like this:
```
# -------------------- MaxFlow Scheduler -------------------- #
Total time usage: 679.69s (1173.64 tokens/s)
Theoretical optimal: 618.59s (1289.56 tokens/s)
# ----------------------------------------------------------- #
```
This log shows the throughput when counting both prompt and decode phase tokens. The value is slightly
lower than the theoretical optimal (i.e. max flow) because of dynamic runtime overheads.

Finally, we also provide examples for other serving modes and request scheduling methods. Please refer
to:
```bash
python step3_run_simulation.py online maxflow
python step3_run_simulation.py <offline/online> <swarm/random/shortest_queue>
```
In online mode, the request arrives based on the distribution in Azure Conversation Dataset. 
`swarm` / `random` / `shortest_queue` are three heuristic-based request scheduling methods we support in
the simulator.

> **Tips:** Our simulator also supports other traces. For arrival rate, we support Azure Conversation
> and Azure Code datasets. For length distribution, we support Azure Conversation, Azure Code, Alpaca,
> and SharedGPT datasets. Please refer to `simulator/trace_generator`. If you want to use these length
> and arrival rate datasets, you need to re-run the profiling (in `simulator/model_manager`) to ensure
> the profiling results are accurate. You also need to change the dataset-related constants in
> `simulator/event_simulator/utils.py`.

## Distributed LLM Serving Real System Tutorial
